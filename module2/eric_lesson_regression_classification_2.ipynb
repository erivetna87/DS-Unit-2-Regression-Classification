{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DS7_lesson_regression_classification_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erivetna87/DS-Unit-2-Regression-Classification/blob/master/module2/eric_lesson_regression_classification_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmOQQWETlITN",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 2*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmFCg6UArzn6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# Regression & Classification, Module 2\n",
        "- Go from simple regression (1 feature) to multiple regression (2+ features)\n",
        "- Get and plot coefficients\n",
        "- Use regression metrics: MAE (Mean Absolute Error) and $R^2$ Score\n",
        "- Understand how ordinary least squares regression minimizes the sum of squared errors\n",
        "- Explain why overfitting is a problem. Do train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLcAHk5arzn8",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtJg1IJZrzn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# If you're in Colab...\n",
        "if in_colab:\n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Regression-Classification.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Install required python packages\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAmYnbqurzn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore this Numpy warning when using Plotly Express:\n",
        "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTMLR9JjmWuT",
        "colab_type": "text"
      },
      "source": [
        "# Go from simple regression (1 feature) to multiple regression (2+ features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLte4DsmmZgC",
        "colab_type": "text"
      },
      "source": [
        "## Overview: Predict Elections! 🗳️"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88JNPm46rzoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "df = pd.read_csv('../data/bread_peace_voting.csv')\n",
        "px.scatter(\n",
        "    df,\n",
        "    x='Average Recent Growth in Personal Incomes',\n",
        "    y='Incumbent Party Vote Share',\n",
        "    text='Year',\n",
        "    title='US Presidential Elections, 1952-2016',\n",
        "    trendline='ols',  # Ordinary Least Squares\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0pqeNP6rzoF",
        "colab_type": "text"
      },
      "source": [
        "#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
        "\n",
        "> Aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
        "\n",
        "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.  \n",
        "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
        "\n",
        "#### Data sources\n",
        "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
        "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
        "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
        "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12\n",
        "\n",
        "> Fatalities denotes the cumulative number of American military fatalities per millions of US population the in Korea, Vietnam, Iraq and Afghanistan wars during the presidential terms preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and 2012 elections. —[Hibbs](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 33"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9grxlv2tzDQ",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNDqNff_0N6m",
        "colab_type": "text"
      },
      "source": [
        "Look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRh-FSMTrzoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_0nPUcQ0RpB",
        "colab_type": "text"
      },
      "source": [
        "What's the average Incumbent Party Vote Share?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjm2CBV1ty35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9fEbVTE0ZMe",
        "colab_type": "text"
      },
      "source": [
        "Add another feature to the scatterplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWNNK5r-t_pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2B1CHml0eJ0",
        "colab_type": "text"
      },
      "source": [
        "Plot the hyperplane of best fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ73YaIF0gJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def regression_3d(df, x, y, z, num=100, **kwargs):\n",
        "    \"\"\"\n",
        "    Visualize linear regression in 3D: 2 features + 1 target\n",
        "    \n",
        "    df : Pandas DataFrame\n",
        "    x : string, feature 1 column in df\n",
        "    y : string, feature 2 column in df\n",
        "    z : string, target column in df\n",
        "    num : integer, number of quantiles for each feature\n",
        "    \"\"\"\n",
        "    \n",
        "    # Plot data\n",
        "    fig = px.scatter_3d(df, x, y, z, **kwargs)\n",
        "    \n",
        "    # Fit Linear Regression\n",
        "    features = [x, y]\n",
        "    target = z\n",
        "    model = LinearRegression()\n",
        "    model.fit(df[features], df[target])    \n",
        "    \n",
        "    # Define grid of coordinates in the feature space\n",
        "    xmin, xmax = df[x].min(), df[x].max()\n",
        "    ymin, ymax = df[y].min(), df[y].max()\n",
        "    xcoords = np.linspace(xmin, xmax, num)\n",
        "    ycoords = np.linspace(ymin, ymax, num)\n",
        "    coords = list(itertools.product(xcoords, ycoords))\n",
        "    \n",
        "    # Make predictions for the grid\n",
        "    predictions = model.predict(coords)\n",
        "    Z = predictions.reshape(num, num).T\n",
        "    \n",
        "    # Plot predictions as a 3D surface (plane)\n",
        "    fig.add_trace(go.Surface(x=xcoords, y=ycoords, z=Z))\n",
        "    \n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2TMPMM4u_5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNoTuf9v7l6A",
        "colab_type": "text"
      },
      "source": [
        "Fit Linear Regression with 2 features\n",
        "\n",
        "\n",
        "- Jake VanderPlas, [_Python Data Science Handbook,_ Chapter 5.2, Introducing Scikit-Learn — Basics of the API](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Basics-of-the-API)\n",
        "- Scikit-Learn documentation, [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj6Jn2x8DdL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Import the appropriate estimator class from Scikit-Learn\n",
        "\n",
        "\n",
        "# 2. Instantiate this class\n",
        "\n",
        "\n",
        "# 3. Arrange X features matrix & y target vector\n",
        "\n",
        "\n",
        "# 4. Fit the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h8Tl7HOwBM7",
        "colab_type": "text"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ490-qEvfSr",
        "colab_type": "text"
      },
      "source": [
        "# Get and plot coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_FZZYFv6G3",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywqSn7hYv71X",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YR1p7eexIf6",
        "colab_type": "text"
      },
      "source": [
        "What's the equation for the hyperplane?\n",
        "\n",
        "Can you relate the intercept and coefficients to what you see in the plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFhJqqSS-i9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAQjaiHoxR-7",
        "colab_type": "text"
      },
      "source": [
        "What if ...\n",
        "\n",
        "Income growth = 0%, fatalities = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rup9q-6gAIZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Itt56qYNxYIb",
        "colab_type": "text"
      },
      "source": [
        "Income growth = 1% (fatalities = 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1onFls9AAh5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5ZMUDjGxdhe",
        "colab_type": "text"
      },
      "source": [
        "The difference between these predictions = ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el1gAUyGApnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U1o9L65xn6_",
        "colab_type": "text"
      },
      "source": [
        "What if... income growth = 2% (fatalities = 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stfnvUc_A3pM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjp2kDm5xq79",
        "colab_type": "text"
      },
      "source": [
        "The difference between these predictions = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zywdu1SJA_hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klON2yUxxu1_",
        "colab_type": "text"
      },
      "source": [
        "What if... (income growth=2%) fatalities = 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTFqg0ixBcIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfqzTR6dxyQ-",
        "colab_type": "text"
      },
      "source": [
        "The difference between these predictions = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtDzQT-Bxel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-3Whu9gx2ac",
        "colab_type": "text"
      },
      "source": [
        "What if income growth = 3% (fatalities = 100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ge-czWaCAWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7wDd61ax6Kk",
        "colab_type": "text"
      },
      "source": [
        "The difference between these predictions = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDWbZ1duCJLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaeJe9px9d1",
        "colab_type": "text"
      },
      "source": [
        "What if (income growth = 3%) fatalities = 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP6UnCT1CXoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuioZsRUyAud",
        "colab_type": "text"
      },
      "source": [
        "The difference between these predictions = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpiDm-QjCZqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkLrZjaDyIOm",
        "colab_type": "text"
      },
      "source": [
        "Plot coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1q748_WCupL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Se1K52yv-u0",
        "colab_type": "text"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LJaGES7vdUs",
        "colab_type": "text"
      },
      "source": [
        "#  Use regression metrics: MAE (Mean Absolute Error) and $R^2$ Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3KyIQI0wGla",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3rm_xvGwHxV",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2UoD6cGkeu",
        "colab_type": "text"
      },
      "source": [
        "How's the error?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAMSfbj0xFv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAhbHPEyGnvp",
        "colab_type": "text"
      },
      "source": [
        "How does this compare to guessing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chGT72Hwz-0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwYWu19BGr9X",
        "colab_type": "text"
      },
      "source": [
        "Plot the \"residuals\" (errors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpQUzQso8FCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "def regression_residuals(df, feature, target, m, b):\n",
        "    \"\"\"\n",
        "    Visualize linear regression, with residual errors,\n",
        "    in 2D: 1 feature + 1 target.\n",
        "    \n",
        "    Use the m & b parameters to \"fit the model\" manually.\n",
        "    \n",
        "    df : Pandas DataFrame\n",
        "    feature : string, feature column in df\n",
        "    target : string, target column in df\n",
        "    m : numeric, slope for linear equation\n",
        "    b : numeric, intercept for linear requation\n",
        "    \"\"\"\n",
        "    \n",
        "    # Plot data\n",
        "    df.plot.scatter(feature, target)\n",
        "    \n",
        "    # Make predictions\n",
        "    x = df[feature]\n",
        "    y = df[target]\n",
        "    y_pred = m*x + b\n",
        "    \n",
        "    # Plot predictions\n",
        "    plt.plot(x, y_pred)\n",
        "    \n",
        "    # Plot residual errors\n",
        "    for x, y1, y2 in zip(x, y, y_pred):\n",
        "        plt.plot((x, x), (y1, y2), color='grey')\n",
        "    \n",
        "    # Print regression metrics\n",
        "    mae = mean_absolute_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    print('Mean Absolute Error:', mae)\n",
        "    print('R^2:', r2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrfbmO5d1GoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7gDItKMwNBH",
        "colab_type": "text"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1_b4j6ZtZgb",
        "colab_type": "text"
      },
      "source": [
        "# Understand how ordinary least squares regression minimizes the sum of squared errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8moJTC6uDuh",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "But Ordinary Least Squares Regression *doesn't* directly minimize MAE or R^2...\n",
        "\n",
        "1. Guess & Check\n",
        "2. Linear Algebra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zspnHmo7wbKF",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebHzgmJfG80i",
        "colab_type": "text"
      },
      "source": [
        "### Guess & Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmdIG_W_8tTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def regression_squared_errors(df, feature, target, m, b):\n",
        "    \"\"\"\n",
        "    Visualize linear regression, with squared errors,\n",
        "    in 2D: 1 feature + 1 target.\n",
        "    \n",
        "    Use the m & b parameters to \"fit the model\" manually.\n",
        "    \n",
        "    df : Pandas DataFrame\n",
        "    feature : string, feature column in df\n",
        "    target : string, target column in df\n",
        "    m : numeric, slope for linear equation\n",
        "    b : numeric, intercept for linear requation\n",
        "    \"\"\"\n",
        "    \n",
        "    # Plot data\n",
        "    fig = plt.figure(figsize=(7,7))\n",
        "    ax = plt.axes()\n",
        "    df.plot.scatter(feature, target, ax=ax)\n",
        "    \n",
        "    # Make predictions\n",
        "    x = df[feature]\n",
        "    y = df[target]\n",
        "    y_pred = m*x + b\n",
        "    \n",
        "    # Plot predictions\n",
        "    ax.plot(x, y_pred)\n",
        "    \n",
        "    # Plot squared errors\n",
        "    xmin, xmax = ax.get_xlim()\n",
        "    ymin, ymax = ax.get_ylim()\n",
        "    scale = (xmax-xmin)/(ymax-ymin)\n",
        "    for x, y1, y2 in zip(x, y, y_pred):\n",
        "        bottom_left = (x, min(y1, y2))\n",
        "        height = abs(y1 - y2)\n",
        "        width = height * scale\n",
        "        ax.add_patch(Rectangle(xy=bottom_left, width=width, height=height, alpha=0.1))\n",
        "    \n",
        "    # Print regression metrics\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y, y_pred)\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    print('Mean Squared Error:', mse)\n",
        "    print('Root Mean Squared Error:', rmse)\n",
        "    print('Mean Absolute Error:', mae)\n",
        "    print('R^2:', r2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkyDSs8f6stD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6biDIhlrzoI",
        "colab_type": "text"
      },
      "source": [
        "### Linear Algebra\n",
        "\n",
        "The same result that is found by minimizing the sum of the squared errors can be also found through a linear algebra process known as the \"Least Squares Solution:\"\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y\n",
        "\\end{align}\n",
        "\n",
        "Before we can work with this equation in its linear algebra form we have to understand how to set up the matrices that are involved in this equation. \n",
        "\n",
        "### The $\\beta$ vector\n",
        "\n",
        "The $\\beta$ vector represents all the parameters that we are trying to estimate, our $y$ vector and $X$ matrix values are full of data from our dataset. The $\\beta$ vector holds the variables that we are solving for: $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "Now that we have all of the necessary parts we can set them up in the following equation:\n",
        "\n",
        "\\begin{align}\n",
        "y = X \\beta + \\epsilon\n",
        "\\end{align}\n",
        "\n",
        "Since our $\\epsilon$ value represents **random** error we can assume that it will equal zero on average.\n",
        "\n",
        "\\begin{align}\n",
        "y = X \\beta\n",
        "\\end{align}\n",
        "\n",
        "The objective now is to isolate the $\\beta$ matrix. We can do this by pre-multiplying both sides by \"X transpose\" $X^{T}$.\n",
        "\n",
        "\\begin{align}\n",
        "X^{T}y =  X^{T}X \\beta\n",
        "\\end{align}\n",
        "\n",
        "Since anything times its transpose will result in a square matrix, if that matrix is then an invertible matrix, then we should be able to multiply both sides by its inverse to remove it from the right hand side. (We'll talk tomorrow about situations that could lead to $X^{T}X$ not being invertible.)\n",
        "\n",
        "\\begin{align}\n",
        "(X^{T}X)^{-1}X^{T}y =  (X^{T}X)^{-1}X^{T}X \\beta\n",
        "\\end{align}\n",
        "\n",
        "Since any matrix multiplied by its inverse results in the identity matrix, and anything multiplied by the identity matrix is itself, we are left with only $\\beta$ on the right hand side:\n",
        "\n",
        "\\begin{align}\n",
        "(X^{T}X)^{-1}X^{T}y = \\hat{\\beta}\n",
        "\\end{align}\n",
        "\n",
        "We will now call it \"beta hat\" $\\hat{\\beta}$ because it now represents our estimated values for $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "### Lets calculate our $\\beta$ coefficients with numpy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouidNhDprzoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is NOT an objective you'll be tested on. It's just a demo.\n",
        "\n",
        "# X is a matrix. Add constant for the intercept.\n",
        "from statsmodels.api import add_constant\n",
        "X = add_constant(df[feature].values)\n",
        "print('X')\n",
        "print(X)\n",
        "\n",
        "# y is a column vector\n",
        "y = df[target].values[:, np.newaxis]\n",
        "print('y')\n",
        "print(y)\n",
        "\n",
        "# Least squares solution in code\n",
        "X_transpose = X.T\n",
        "X_transpose_X = X_transpose @ X\n",
        "X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
        "X_transpose_y = X_transpose @ y\n",
        "beta_hat = X_transpose_X_inverse @ X_transpose_y\n",
        "\n",
        "print('Beta Hat')\n",
        "print(beta_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDTgv1lKrzoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scikit-learn gives the exact same results\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "features = ['Average Recent Growth in Personal Incomes']\n",
        "target = 'Incumbent Party Vote Share'\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "model.fit(X, y)\n",
        "model.intercept_, model.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkkW-kEpwdq6",
        "colab_type": "text"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLUPbonrzoM",
        "colab_type": "text"
      },
      "source": [
        "# Explain why overfitting is a problem. Do train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-40FQscwfD5",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9Aboy6MrzoR",
        "colab_type": "text"
      },
      "source": [
        "#### Jake VanderPlas, [_Python Data Science Handbook,_ Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off)\n",
        "\n",
        "Fundamentally, the question of \"the best model\" is about finding a sweet spot in the tradeoff between bias and variance. Consider the following figure, which presents two regression fits to the same dataset:\n",
        "\n",
        "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-bias-variance-2.png)\n",
        "\n",
        "The model on the left attempts to find a straight-line fit through the data. Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well. Such a model is said to _underfit_ the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high _bias_.\n",
        "\n",
        "The model on the right attempts to fit a high-order polynomial through the data. Here the model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data. Such a model is said to _overfit_ the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high _variance_.\n",
        "\n",
        "\n",
        "From the scores associated with these two models, we can make an observation that holds more generally:\n",
        "\n",
        "- For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n",
        "\n",
        "- For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.\n",
        "\n",
        "If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in the following figure:\n",
        "\n",
        "![](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png)\n",
        "\n",
        "The diagram shown here is often called a validation curve, and we see the following essential features:\n",
        "\n",
        "- The training score is everywhere higher than the validation score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.\n",
        "- For very low model complexity (a high-bias model), the training data is under-fit, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
        "- For very high model complexity (a high-variance model), the training data is over-fit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
        "- For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance.\n",
        "\n",
        "The means of tuning the model complexity varies from model to model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ8rflrBwgYj",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ajsC5CS9nYI",
        "colab_type": "text"
      },
      "source": [
        "Wrangle New York City property sales data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzCSC1zsrzoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read New York City property sales data, from first 4 months of 2019.\n",
        "# Dataset has 23040 rows, 21 columns.\n",
        "df = pd.read_csv('../data/NYC_Citywide_Rolling_Calendar_Sales.csv')\n",
        "assert df.shape == (23040, 21)\n",
        "\n",
        "# Change column names. Replace spaces with underscores\n",
        "df.columns = [col.replace(' ', '_') for col in df]\n",
        "\n",
        "# Remove symbols from SALE_PRICE string, convert to integer\n",
        "df['SALE_PRICE'] = (\n",
        "    df['SALE_PRICE']\n",
        "    .str.replace('$','')\n",
        "    .str.replace('-','')\n",
        "    .str.replace(',','')\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# Keep subset of rows:\n",
        "# Tribeca neighborhood, Condos - Elevator Apartments, \n",
        "# 1 unit, sale price more than $1, less than $35 million\n",
        "mask = (\n",
        "    (df['NEIGHBORHOOD'].str.contains('TRIBECA')) & \n",
        "    (df['BUILDING_CLASS_CATEGORY'] == '13 CONDOS - ELEVATOR APARTMENTS') &\n",
        "    (df['TOTAL_UNITS'] == 1) & \n",
        "    (df['SALE_PRICE'] > 0) & \n",
        "    (df['SALE_PRICE'] < 35000000)\n",
        ")\n",
        "df = df[mask]\n",
        "\n",
        "# Data now has 90 rows, 21 columns\n",
        "assert df.shape == (90, 21)\n",
        "\n",
        "# Convert SALE_DATE to datetime\n",
        "df['SALE_DATE'] = pd.to_datetime(df['SALE_DATE'], infer_datetime_format=True)\n",
        "\n",
        "from ipywidgets import interact\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Read New York City property sales data, from first 4 months of 2019.\n",
        "# Dataset has 23040 rows, 21 columns.\n",
        "df = pd.read_csv('../data/NYC_Citywide_Rolling_Calendar_Sales.csv')\n",
        "assert df.shape == (23040, 21)\n",
        "\n",
        "# Change column names. Replace spaces with underscores\n",
        "df.columns = [col.replace(' ', '_') for col in df]\n",
        "\n",
        "# Remove symbols from SALE_PRICE string, convert to integer\n",
        "df['SALE_PRICE'] = (\n",
        "    df['SALE_PRICE']\n",
        "    .str.replace('$','')\n",
        "    .str.replace('-','')\n",
        "    .str.replace(',','')\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# Keep subset of rows:\n",
        "# Tribeca neighborhood, Condos - Elevator Apartments, \n",
        "# 1 unit, sale price more than $1, less than $35 million\n",
        "mask = (\n",
        "    (df['NEIGHBORHOOD'].str.contains('TRIBECA')) & \n",
        "    (df['BUILDING_CLASS_CATEGORY'] == '13 CONDOS - ELEVATOR APARTMENTS') &\n",
        "    (df['TOTAL_UNITS'] == 1) & \n",
        "    (df['SALE_PRICE'] > 0) & \n",
        "    (df['SALE_PRICE'] < 35000000)\n",
        ")\n",
        "df = df[mask]\n",
        "\n",
        "# Data now has 90 rows, 21 columns\n",
        "assert df.shape == (90, 21)\n",
        "\n",
        "# Convert SALE_DATE to datetime\n",
        "df['SALE_DATE'] = pd.to_datetime(df['SALE_DATE'], infer_datetime_format=True)\n",
        "\n",
        "# Arrange X features matrix & y target vector\n",
        "features = ['GROSS_SQUARE_FEET']\n",
        "target = 'SALE_PRICE'\n",
        "X = df[features]\n",
        "y = df[target]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VccY7fno9sA8",
        "colab_type": "text"
      },
      "source": [
        "Do random train/test spliit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki0qmlk69uWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQnoWJV9zkr",
        "colab_type": "text"
      },
      "source": [
        "Repeatedly fit increasingly complex models, and keep track of the scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "YShWjZrIrzoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# Credit: Jake VanderPlas, Python Data Science Handbook, Chapter 5.3\n",
        "# https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#Validation-curves-in-Scikit-Learn\n",
        "def PolynomialRegression(degree=2, **kwargs):\n",
        "    return make_pipeline(PolynomialFeatures(degree), \n",
        "                         LinearRegression(**kwargs))\n",
        "\n",
        "\n",
        "polynomial_degrees = range(1, 10, 2)\n",
        "train_r2s = []\n",
        "test_r2s = []\n",
        "\n",
        "for degree in polynomial_degrees:\n",
        "    model = PolynomialRegression(degree)\n",
        "    display(HTML(f'Polynomial degree={degree}'))\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    train_r2 = model.score(X_train, y_train)\n",
        "    test_r2 = model.score(X_test, y_test)\n",
        "    display(HTML(f'<b style=\"color: blue\">Train R2 {train_r2:.2f}</b>'))\n",
        "    display(HTML(f'<b style=\"color: red\">Test R2 {test_r2:.2f}</b>'))\n",
        "\n",
        "    plt.scatter(X_train, y_train, color='blue', alpha=0.5)\n",
        "    plt.scatter(X_test, y_test, color='red', alpha=0.5)\n",
        "    plt.xlabel(features)\n",
        "    plt.ylabel(target)\n",
        "    \n",
        "    x_domain = np.linspace(X.min(), X.max())\n",
        "    curve = model.predict(x_domain)\n",
        "    plt.plot(x_domain, curve, color='blue')\n",
        "    plt.show()\n",
        "    display(HTML('<hr/>'))\n",
        "    \n",
        "    train_r2s.append(train_r2)\n",
        "    test_r2s.append(test_r2)\n",
        "    \n",
        "display(HTML('Validation Curve'))\n",
        "plt.plot(polynomial_degrees, train_r2s, color='blue', label='Train')\n",
        "plt.plot(polynomial_degrees, test_r2s, color='red', label='Test')\n",
        "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
        "plt.ylabel('R^2 Score')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWkI7C7-wllP",
        "colab_type": "text"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMeK2F8OwoOE",
        "colab_type": "text"
      },
      "source": [
        "# Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WcLE44XwpOL",
        "colab_type": "text"
      },
      "source": [
        "# Sources\n",
        "\n",
        "Jake VanderPlas, [_Python Data Science Handbook,_ Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off)\n",
        "\n",
        "\n",
        "#### Douglas Hibbs, [Background Information on the ‘Bread and Peace’ Model of Voting in Postwar US Presidential Elections](https://douglas-hibbs.com/background-information-on-bread-and-peace-voting-in-us-presidential-elections/)\n",
        "\n",
        "> Aggregate two-party vote shares going to candidates of the party holding the presidency during the postwar era are well explained by just two fundamental determinants:\n",
        "\n",
        "> (1) Positively by weighted-average growth of per capita real disposable personal income over the term.  \n",
        "> (2) Negatively by cumulative US military fatalities (scaled to population) owing to unprovoked, hostile deployments of American armed forces in foreign wars.\n",
        "\n",
        "#### Data sources\n",
        "- 1952-2012: Douglas Hibbs, [2014 lecture at Deakin University Melbourne](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 40\n",
        "- 2016, Vote Share: [The American Presidency Project](https://www.presidency.ucsb.edu/statistics/elections)\n",
        "- 2016, Recent Growth in Personal Incomes: [The 2016 election economy: the \"Bread and Peace\" model final forecast](https://angrybearblog.com/2016/11/the-2016-election-economy-the-bread-and-peace-model-final-forecast.html)\n",
        "- 2016, US Military Fatalities: Assumption that Afghanistan War fatalities in 2012-16 occured at the same rate as 2008-12\n",
        "\n",
        "> Fatalities denotes the cumulative number of American military fatalities per millions of US population the in Korea, Vietnam, Iraq and Afghanistan wars during the presidential terms preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and 2012 elections. —[Hibbs](http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf), Slide 33"
      ]
    }
  ]
}